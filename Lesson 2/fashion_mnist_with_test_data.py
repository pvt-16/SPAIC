# -*- coding: utf-8 -*-
"""fashion mnist_with test data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PJ1OOzALpfaU7kge4bgtNb6nn7V2kuHs
"""

# Import libraries
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch import optim

# Defined by Udacity

def view_classify(img, ps, version="Fashion"):
    ''' Function for viewing an image and it's predicted classes.
    '''
    ps = ps.data.numpy().squeeze()

    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)
    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())
    ax1.axis('off')
    ax2.barh(np.arange(10), ps)
    ax2.set_aspect(0.1)
    ax2.set_yticks(np.arange(10))
    if version == "MNIST":
        ax2.set_yticklabels(np.arange(10))
    elif version == "Fashion":
        ax2.set_yticklabels(['T-shirt/top',
                            'Trouser',
                            'Pullover',
                            'Dress',
                            'Coat',
                            'Sandal',
                            'Shirt',
                            'Sneaker',
                            'Bag',
                            'Ankle Boot'], size='small');
    ax2.set_title('Class Probability')
    ax2.set_xlim(0, 1.1)

plt.tight_layout()

# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,)),])

# Download and load the training data
trainset = datasets.FashionMNIST('~/.pytorch/Fashion-MNIST_data/', download=True, train=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

# Download and load test data
testset = datasets.FashionMNIST('~/.pytorch/Fashion-MNIST_data/', download=True, train=False, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)

#Define model
   
model = nn.Sequential(nn.Linear(784, 128),
                     nn.ReLU(),
                     nn.Linear(128, 64),
                     nn.ReLU(),
                     nn.Linear(64, 10),
                     nn.LogSoftmax(dim=1))

"""class model(nn.Module):
  #hidden layers
   def __init__(self):
      super().__init__()
      self.hidden_layer_1 = nn.Linear(784, 256)
      self.hidden_layer_2 = nn.Linear(256, 128)
      self.ouput_layer = nn.Linear(128,10)
  
   def forward(self,x):
      x = F.relu(self.hidden_layer_1(x))
      x = F.relu(self.hidden_layer_2(x))    
      x = F.LogSoftmax(x)
      
      return x
"""

#training_model = model()
#Define loss
#CrossEntropyLoss this time
criterion= nn.NLLLoss()

#Define optimizer
optimizer = optim.SGD(model[0].parameters(), lr=0.01)

#preparing data for network
images, labels = next(iter(trainloader))

#one pass through network

def one_pass(images, labels, running_loss:float):
  
  #set the gradients to zero
  optimizer.zero_grad()
  
  #Prepare data (images) for the model
  flattened_images = images.view(images.shape[0],-1)
  #flattened_images.requires_grad= True
  
  #Forward pass. Output of logsoftmax
  log_probabilities = model(flattened_images)
  
  #calculate loss after getting output
  loss = criterion(log_probabilities, labels)
  
  #Calculate loss gradients after getting output
  grads = loss.backward()
  
  #add gradients to weights
  optimizer.step()
  
  running_loss += loss.item()
  print(running_loss)
  return running_loss

one_pass(images, labels, 0)

#multiple pass through network

def mul_pass(running_loss=0):
  
  for images, labels in trainloader:
  #set the gradients to zero
    optimizer.zero_grad()
  
  #Prepare data (images) for the model
    flattened_images = images.view(images.shape[0],-1)
    #flattened_images.requires_grad= True
  
  #Forward pass. Output of logsoftmax
    log_probabilities = model(flattened_images)
  
  #calculate loss after getting output
    loss = criterion(log_probabilities, labels)
  
  #Calculate loss gradients after getting output
    grads = loss.backward()
  
  #add gradients to weights
    optimizer.step()
  
    running_loss += loss.item()
  else:
    print(f"Training loss: {running_loss/len(trainloader)}")

def mul_pass_test(running_loss=0):
  
  for images, labels in testloader:
  #set the gradients to zero
    optimizer.zero_grad()
  
  #Prepare data (images) for the model
    flattened_images = images.view(images.shape[0],-1)
    #flattened_images.requires_grad= True
  
  #Forward pass. Output of logsoftmax
    log_probabilities = model(flattened_images)
  
  #calculate loss after getting output
    loss = criterion(log_probabilities, labels)
  
  #Calculate loss gradients after getting output
    grads = loss.backward()
  
  #add gradients to weights
    optimizer.step()
  
    running_loss += loss.item()
  else:
    print(f"Training loss: {running_loss/len(trainloader)}")

epochs = 15

for i in range(epochs):
  mul_pass()

#ignore this bit
#  for images, labels in trainloader:
#    run_loss = one_pass(images, labels, running_loss)
 #   running_loss = run_loss
 # else:
#    print(f"Training loss: {running_loss/len(trainloader)}")

images, labels = next(iter(trainloader))

img = images[0].view(1, 784)
# Turn off gradients to speed up this part
with torch.no_grad():
    logits = model.forward(img)

# Output of the network are logits, need to take softmax for probabilities
ps = nn.functional.softmax(logits, dim=1)
view_classify(img.view(1, 28, 28), ps)

#cnn.eval()
correct = 0
total = 0
for images, labels in testloader:
 #   images = Variable(images.float())
    flattened_images = images.view(images.shape[0],-1)
    outputs = model(flattened_images)
    _, predicted = torch.max(outputs.data, 1)
    total += labels.size(0)
    correct += (predicted == labels).sum()

print('Test Accuracy of the model on the 10000 test images: %.4f %%' % (100 * correct / total))

